{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36ae73e-c992-472a-b143-21815491fe37",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc8b31fb-d35a-4a34-9fea-37b3e19d31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00efeb3-ac79-411a-86c3-8cab2e071733",
   "metadata": {},
   "source": [
    " ## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835aed06-a953-4f26-94a7-7211ea8ac8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nisha/.cache\\torch\\hub\\pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "yolo = YOLO(r\"C:\\Users\\nisha\\OneDrive\\Desktop\\ML Projects\\Kalkini_Project\\models\\yolov8n.pt\")  # YOLOv8 for person detection\n",
    "deepsort_tracker_cam1 = DeepSort(max_age=30)  # DeepSORT tracker for Camera 1\n",
    "deepsort_tracker_cam2 = DeepSort(max_age=30)  # DeepSORT tracker for Camera 2\n",
    "\n",
    "# Initialize ReID model with updated weights syntax\n",
    "class ReIDModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision', 'resnet50', weights='DEFAULT')\n",
    "        self.model.eval()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = self.model.avgpool(x)\n",
    "        return x.flatten(1)\n",
    "\n",
    "reid_model = ReIDModel().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77f997-f0fb-4ce4-b1fa-106cd5a1f83f",
   "metadata": {},
   "source": [
    "## Open video streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12b1671-1548-4360-804a-b16213e13356",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1 = cv2.VideoCapture(r\"C:\\Users\\nisha\\OneDrive\\Desktop\\ML Projects\\Kalkini_Project\\input_videos\\test.mp4\")  # Camera 1\n",
    "cap2 = cv2.VideoCapture(r\"C:\\Users\\nisha\\OneDrive\\Desktop\\ML Projects\\Kalkini_Project\\input_videos\\test.mp4\")  # Camera 2\n",
    "\n",
    "# Global storage for cross-camera ReID features\n",
    "global_reid_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef2c63-a3c0-461f-855c-4a240c64b724",
   "metadata": {},
   "source": [
    "## Functions Defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98411117-b915-46e3-b49e-85ac35496b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reid_embedding(crop):\n",
    "    \"\"\"Extract ReID embedding from a cropped image.\"\"\"\n",
    "    crop = cv2.resize(crop, (128, 256))  # Resize to ReID input size\n",
    "    crop = torch.from_numpy(crop).permute(2, 0, 1).float().div(255).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        embedding = reid_model(crop).squeeze().cpu().numpy()\n",
    "    return embedding / np.linalg.norm(embedding)  # Normalize\n",
    "\n",
    "def process_camera(cap, tracker, camera_id):\n",
    "    \"\"\"Process a single camera feed and return tracked persons with embeddings and frame.\"\"\"\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return [], None  # Return None for frame if read fails\n",
    "    \n",
    "    # Detect people with YOLOv8\n",
    "    results = yolo.predict(frame, classes=[0], verbose=False)  # Class 0 = person\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confs = results[0].boxes.conf.cpu().numpy()\n",
    "    \n",
    "    # Prepare DeepSORT detections\n",
    "    detections = []\n",
    "    for box, conf in zip(boxes, confs):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        detections.append(([x1, y1, x2, y2], conf, \"person\"))\n",
    "    \n",
    "    # Update DeepSORT tracker\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "    \n",
    "    # Extract ReID embeddings for tracked persons\n",
    "    tracked_persons = []\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        crop = frame[y1:y2, x1:x2]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract ReID embedding\n",
    "        embedding = extract_reid_embedding(crop)\n",
    "        tracked_persons.append((track_id, embedding, (x1, y1, x2, y2)))\n",
    "    \n",
    "    return tracked_persons, frame  # Return both tracked persons and frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc19dd3-4a32-4a62-84dc-2adb1e7beabb",
   "metadata": {},
   "source": [
    "## Main LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0522e5-1d99-4d37-8d5d-6bc62fc662e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Process Camera 1 and get its frame\n",
    "    persons_cam1, frame1 = process_camera(cap1, deepsort_tracker_cam1, camera_id=1)\n",
    "    # Process Camera 2 and get its frame\n",
    "    persons_cam2, frame2 = process_camera(cap2, deepsort_tracker_cam2, camera_id=2)\n",
    "    \n",
    "    # Skip iteration if frames are None (e.g., camera disconnected)\n",
    "    if frame1 is None or frame2 is None:\n",
    "        continue\n",
    "    \n",
    "    # Check for cross-camera matches using ReID embeddings\n",
    "    for p1_id, p1_embed, (x1, y1, x2, y2) in persons_cam1:\n",
    "        for p2_id, p2_embed, _ in persons_cam2:\n",
    "            similarity = np.dot(p1_embed, p2_embed)\n",
    "            if similarity > 0.7:  # Threshold (adjust based on your use case)\n",
    "                print(f\"Person {p1_id} (Cam1) matches Person {p2_id} (Cam2)!\")\n",
    "                # Highlight bounding boxes in both cameras\n",
    "                cv2.rectangle(frame1, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame2, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display frames\n",
    "    cv2.imshow(\"Camera 1\", frame1)\n",
    "    cv2.imshow(\"Camera 2\", frame2)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edf645-29eb-42d2-b3ac-f37f1dd9561a",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a80e4a09-79e7-4db2-82d3-7c8d6a3e01f6",
   "metadata": {},
   "source": [
    "Open source references used:\n",
    "1. https://github.com/parmarkashish/person-reidentification-system\n",
    "Youtube Video Link: https://youtu.be/iTO70oF4r9Q?si=vAY-t4c_gryejnXc\n",
    "2. https://github.com/noorkhokhar99/people-tracking-yolov7/blob/main/README.md\n",
    "3. Youtube Video Link : https://youtu.be/COYUiWxthMc?si=qGzA09O7WMUEpOYo\n",
    "4. https://github.com/AarohiSingla/Tracking-and-counting-Using-YOLOv8-and-DeepSORT\n",
    "5. Youtube Video Link : https://youtu.be/l_kf9CfZ_8M?si=M8HlJs2b9wS4xx4c\n",
    "6. https://github.com/freedomwebtech/yolov8-students-counting-lobby"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf250f8a-4fbb-47fd-96b4-63720e998b29",
   "metadata": {},
   "source": [
    "## More Features to ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29bb22-9cbb-4937-866c-683fc78bf5fc",
   "metadata": {},
   "source": [
    "### 1. Search Person by ID and Highlight Movement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525942ad-5888-4c83-a2ba-5cf7b09f338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_person_trail(frame, track_history, person_id, color=(0, 0, 255)):\n",
    "    \"\"\"\n",
    "    Highlights the movement trail of a specific person.\n",
    "\n",
    "    :param frame: The current video frame.\n",
    "    :param track_history: Dictionary storing movement history {id: [(x, y), ...]}\n",
    "    :param person_id: The ID of the person to highlight.\n",
    "    :param color: The color for the highlighted trail.\n",
    "    \"\"\"\n",
    "    if person_id in track_history:\n",
    "        path = track_history[person_id]\n",
    "        \n",
    "        # Draw trail\n",
    "        for i in range(1, len(path)):\n",
    "            cv2.line(frame, path[i - 1], path[i], color, 3)\n",
    "\n",
    "        # Draw last known position\n",
    "        if path:\n",
    "            cv2.circle(frame, path[-1], 6, color, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530b8f3-5f9a-4f9a-aae4-09c4c56672e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_person_movement(track_history, person_id):\n",
    "    \"\"\"\n",
    "    Plots the movement history of a given person ID.\n",
    "\n",
    "    :param track_history: Dictionary storing movement history {id: [(x, y), ...]}\n",
    "    :param person_id: The ID of the person to visualize.\n",
    "    \"\"\"\n",
    "    if person_id not in track_history:\n",
    "        print(f\"No movement data for ID {person_id}\")\n",
    "        return\n",
    "    \n",
    "    x_vals, y_vals = zip(*track_history[person_id])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x_vals, y_vals, marker='o', linestyle='-', color='blue', label=f\"ID {person_id} Movement\")\n",
    "    plt.scatter(x_vals[-1], y_vals[-1], color='red', label=\"Last Seen Position\", zorder=3)\n",
    "    \n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "    plt.title(f\"Movement History of ID {person_id}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27dc8bf-cad2-408e-a63f-c95422257431",
   "metadata": {},
   "source": [
    "### 2. Implementing an Automatic Alert System for Restricted Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027bf4d-4ae4-4e0c-98ed-1fe4e57e94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_zones = [\n",
    "    (100, 100, 400, 400),  # restricted area 1\n",
    "    (500, 200, 700, 500)   # restricted area 2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32e4c8-a38c-415b-b039-069908951c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "def check_restricted_area(frame, track_history, restricted_zones, alert_ids):\n",
    "    \"\"\"\n",
    "    Checks if a person enters a restricted area and triggers an alert.\n",
    "\n",
    "    :param frame: The current video frame.\n",
    "    :param track_history: Dictionary storing movement history {id: [(x, y), ...]}\n",
    "    :param restricted_zones: List of predefined restricted areas.\n",
    "    :param alert_ids: Set of person IDs to be monitored.\n",
    "    \"\"\"\n",
    "    for person_id, path in track_history.items():\n",
    "        if not path:\n",
    "            continue\n",
    "\n",
    "        # Get the person's latest position (center of bounding box)\n",
    "        x, y = path[-1]  \n",
    "\n",
    "        # Check if inside any restricted zone\n",
    "        for (x1, y1, x2, y2) in restricted_zones:\n",
    "            if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                print(f\"ALERT! Person ID {person_id} entered a restricted area!\")\n",
    "                \n",
    "                # Play a beep sound (Windows)\n",
    "                winsound.Beep(1000, 500)  # Frequency: 1000 Hz, Duration: 500ms\n",
    "\n",
    "                # Highlight the person in red\n",
    "                cv2.circle(frame, (x, y), 8, (0, 0, 255), -1)\n",
    "                cv2.putText(frame, f'ALERT ID {person_id}', (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "                # Stop checking once alert is triggered\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595f1df-550f-4c71-b842-d4727385ad44",
   "metadata": {},
   "source": [
    "### 3. Facial Identification for Verifying Individuals from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c23e8a-3cc7-4fec-b811-7dec9c84c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_face(frame, bbox):\n",
    "    \"\"\"\n",
    "    Extracts the face region from a person's bounding box.\n",
    "\n",
    "    :param frame: The video frame.\n",
    "    :param bbox: Tuple (x1, y1, x2, y2) representing the bounding box.\n",
    "    :return: Cropped face image or None if extraction fails.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    face_region = frame[y1:y2, x1:x2]\n",
    "\n",
    "    if face_region.shape[0] > 0 and face_region.shape[1] > 0:\n",
    "        return cv2.resize(face_region, (160, 160))  # Resize for FaceNet/DeepFace\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2384d-2284-4888-9aa5-88137e249776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "\n",
    "face_db = {}  # Stores embeddings for known people\n",
    "\n",
    "def identify_person(face_crop):\n",
    "    \"\"\"\n",
    "    Identifies a person using facial recognition.\n",
    "\n",
    "    :param face_crop: Cropped face image.\n",
    "    :return: Person ID if recognized, None otherwise.\n",
    "    \"\"\"\n",
    "    if face_crop is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        embedding = DeepFace.represent(face_crop, model_name=\"Facenet\")[0][\"embedding\"]\n",
    "\n",
    "        # Compare with stored embeddings\n",
    "        for person_id, ref_embedding in face_db.items():\n",
    "            distance = np.linalg.norm(np.array(embedding) - np.array(ref_embedding))\n",
    "            if distance < 0.6:  # Threshold for FaceNet similarity\n",
    "                return person_id\n",
    "\n",
    "        return None\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede6851-9a9b-4c1f-9402-0251f72fe266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_face(person_id, face_crop):\n",
    "    \"\"\"\n",
    "    Stores a new person's face embedding in the database.\n",
    "\n",
    "    :param person_id: Unique ID assigned to the person.\n",
    "    :param face_crop: Cropped face image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = DeepFace.represent(face_crop, model_name=\"Facenet\")[0][\"embedding\"]\n",
    "        face_db[person_id] = embedding\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa29261-5ca0-4708-8dfe-db147dfdd638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
